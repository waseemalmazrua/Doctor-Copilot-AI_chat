from fastapi import FastAPI, UploadFile, File, Request
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from openai import OpenAI
import tensorflow as tf
import numpy as np
from PIL import Image, ImageOps
import os
import uuid
import io
import base64
import json

app = FastAPI(title="Dr. Copilot", version="2.0")

# ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ù…Ù„Ù .env
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")

print("ğŸ¯ Using configuration from .env file")
print(f"ğŸ”‘ API Key: {'âœ…' if api_key and api_key.startswith('sk-') else 'âŒ'}")

# ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
os.makedirs("static/uploads", exist_ok=True)
os.makedirs("static/masks", exist_ok=True)
os.makedirs("templates", exist_ok=True)

app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")


# ğŸ§  ØªØ¹Ø±ÙŠÙ Ø¯ÙˆØ§Ù„ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
def dice_coef(y_true, y_pred, smooth=1):
    y_true_f = tf.keras.backend.flatten(y_true)
    y_pred_f = tf.keras.backend.flatten(y_pred)
    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)


def iou(y_true, y_pred, smooth=1):
    intersection = tf.keras.backend.sum(tf.keras.backend.abs(y_true * y_pred), axis=-1)
    union = tf.keras.backend.sum(y_true, -1) + tf.keras.backend.sum(y_pred, -1) - intersection
    return (intersection + smooth) / (union + smooth)


# ğŸ¤– Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI
openai_client = None
if api_key and api_key.startswith('sk-'):
    try:
        openai_client = OpenAI(api_key=api_key)
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„
        openai_client.models.list()
        print("âœ… OpenAI client configured successfully!")
    except Exception as e:
        print(f"âŒ OpenAI client failed: {e}")
        openai_client = None
else:
    print("âŒ OpenAI not configured properly")

# ğŸ§  ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = None
try:
    model_path = "model/my_model_waseem_finetuned_50.keras"
    if os.path.exists(model_path):
        # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹ Ø¯ÙˆØ§Ù„ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ø®ØµØµØ©
        model = tf.keras.models.load_model(
            model_path,
            custom_objects={
                'dice_coef': dice_coef,
                'iou': iou
            },
            compile=False
        )
        print("âœ… Model loaded successfully!")
        print(f"ğŸ“Š Model input shape: {model.input_shape}")
    else:
        print(f"âŒ Model file not found: {model_path}")
        model_dir = "model"
        if os.path.exists(model_dir):
            available_models = os.listdir(model_dir)
            print(f"ğŸ“ Available models: {available_models}")
except Exception as e:
    print(f"âŒ Model failed to load: {e}")


# ğŸ†• ğŸ”¥ Ø£Ø¶Ù Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù‡Ù†Ø§ - Ø¥Ø±Ø³Ø§Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯ AI
async def analyze_with_ai_assistant(image_path: str, analysis_results: dict):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø© ÙˆÙ†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯ AI"""
    try:
        if not openai_client:
            print("âŒ OpenAI client not available")
            return "AI assistant not available"

        print(f"ğŸ”„ Sending image to AI assistant: {image_path}")

        # âœ… Ø£Ø±Ø³Ù„ base64 Ø¨Ø¯ÙˆÙ† data URL header
        with open(image_path, "rb") as image_file:
            image_data = image_file.read()
            base64_image = base64.b64encode(image_data).decode('utf-8')

        print("âœ… Image converted to base64")

        # Ø¨Ù†Ø§Ø¡ Ø±Ø³Ø§Ù„Ø© Ù…ÙØµÙ„Ø©
        analysis_context = f"""
        Neural Segmentation Analysis has detected the following:
        - Detection Percentage: {analysis_results['detection_percentage']}% of scan area
        - Status: {analysis_results['status']}
        - Color Indicator: {analysis_results['color']}

        Please provide a comprehensive educational analysis of this medical image:

        1. Describe the anatomical structures visible
        2. Explain any notable features or patterns
        3. Provide educational context about what these findings might represent
        4. Suggest what a medical professional would consider
        5. Emphasize the importance of clinical correlation

        Focus on descriptive, educational insights rather than definitive diagnoses.
        """

        messages = [
            {
                "role": "system",
                "content": """You are Dr. Copilot, a medical imaging education assistant. 
                Provide detailed, descriptive analysis of medical images for educational purposes.
                Focus on anatomical description, feature explanation, and clinical context.

                âœ… WHAT YOU CAN AND SHOULD DO:
                - Describe visible anatomical structures in detail
                - Explain imaging features, textures, and patterns
                - Identify normal vs. abnormal appearances
                - Provide educational insights about medical imaging
                - Suggest what medical professionals look for in such scans
                - Explain the clinical significance of various findings
                - Guide users on appropriate next steps

                ğŸš« REMINDERS:
                - You are an educational assistant, not a diagnostic tool
                - Always recommend professional medical consultation
                - Focus on descriptive analysis rather than definitive conclusions
                - Use clear, accessible language for medical education"""
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": analysis_context},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]
            }
        ]

        print("ğŸ”„ Calling OpenAI API...")
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=1500
        )

        return response.choices[0].message.content

    except Exception as e:
        print(f"âŒ AI assistant error: {e}")
        return f"AI analysis failed: {str(e)}"


@app.get("/")
async def home(request: Request):
    return templates.TemplateResponse("index.html", {
        "request": request,
        "openai_configured": openai_client is not None
    })


@app.get("/api/health")
async def health_check():
    return JSONResponse({
        "status": "healthy",
        "model_loaded": model is not None,
        "openai_configured": openai_client is not None,
        "config": {
            "api_key_configured": bool(api_key and api_key.startswith('sk-'))
        }
    })


@app.post("/api/analyze")
async def analyze_image(file: UploadFile = File(...)):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± - Ù…Ø¹ Ø¥Ø±Ø³Ø§Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯ AI"""
    try:
        if not file.content_type.startswith("image/"):
            return JSONResponse({"success": False, "error": "ÙŠØ±Ø¬Ù‰ Ø±ÙØ¹ Ù…Ù„Ù ØµÙˆØ±Ø©"})

        content = await file.read()
        if len(content) == 0:
            return JSONResponse({"success": False, "error": "Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº"})

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©
        image = Image.open(io.BytesIO(content)).convert("RGB")

        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
        unique_id = str(uuid.uuid4())[:8]
        original_filename = f"original_{unique_id}.png"
        original_path = f"static/uploads/{original_filename}"
        image.save(original_path)

        # ğŸ”¥ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
        detection_percent = 0
        status = "Not Analyzed"
        color = "#007bff"
        overlay_image = image
        mask_resized = Image.new("L", image.size, 0)

        if model:
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (256x256)
                processed = image.resize((256, 256))
                img_array = np.array(processed) / 255.0
                img_array = np.expand_dims(img_array, axis=0)

                # Ø§Ù„ØªÙ†Ø¨Ø¤
                prediction = model.predict(img_array, verbose=0)

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø§Ø³Ùƒ Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤
                if len(prediction.shape) == 4:
                    pred_mask = prediction[0, ..., 0]
                else:
                    pred_mask = prediction[0]

                print(f"ğŸ“Š Prediction range: [{pred_mask.min():.3f}, {pred_mask.max():.3f}]")
                print(f"ğŸ“ˆ Prediction mean: {pred_mask.mean():.3f}")

                # Ø§Ù„Ø¹ØªØ¨Ø© Ø§Ù„Ø°ÙƒÙŠØ©
                threshold = 0.3
                binary_mask = (pred_mask > threshold).astype(np.uint8)

                # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
                mask_area = np.sum(binary_mask)
                total_area = binary_mask.size
                detection_percent = (mask_area / total_area) * 100

                print(f"ğŸ¯ Using threshold: {threshold}")
                print(f"ğŸ“¦ Mask pixels: {mask_area}/{total_area} ({detection_percent:.2f}%)")

                # ğŸ”¥ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù€ overlay - Ø¨Ø¯ÙˆÙ† cv2
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø§Ù„Ù…Ù„ÙˆÙ†
                # ğŸ”¥ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù€ overlay (Ù†Ø³Ø®Ø© ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Cloud Run Ùˆ localhost)
                mask_resized = Image.fromarray((binary_mask * 255).astype(np.uint8)).resize(image.size)
                binary_mask_resized = np.array(mask_resized) > 0

                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© NumPy
                original_array = np.array(image.convert("RGB"))

                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø¥Ù„Ù‰ BGR Ù„ØªØ¬Ù†Ø¨ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙÙŠ Ø¨ÙŠØ¦Ø§Øª headless
                original_bgr = original_array[..., ::-1]

                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø§Ø³Ùƒ Ù…Ù„ÙˆÙ† Ø¨Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø­Ù…Ø±
                red_mask = np.zeros_like(original_bgr)
                red_mask[binary_mask_resized] = [0, 0, 255]  # Ø£Ø­Ù…Ø± (BGR)

                # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø§Ø³Ùƒ Ù…Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                alpha = 0.6
                overlay_bgr = (original_bgr * (1 - alpha) + red_mask * alpha).astype(np.uint8)

                # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ RGB Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØµØ­ÙŠØ­ ÙÙŠ Ø§Ù„Ù…ØªØµÙØ­
                overlay_rgb = overlay_bgr[..., ::-1]
                overlay_image = Image.fromarray(overlay_rgb)

                # ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù
                if detection_percent < 2.0:
                    status = "Normal"
                    color = "#00ff00"
                elif detection_percent < 10.0:
                    status = "Minor Findings"
                    color = "#ffff00"
                else:
                    status = "Significant Findings"
                    color = "#ff0000"

            except Exception as model_error:
                print(f"âŒ Model prediction error: {model_error}")
                detection_percent = 0
                status = "Analysis Failed"
                color = "#ff0000"

        # ğŸ”¥ ğŸ†• Ø§Ù„Ø¬Ø¯ÙŠØ¯: Ø¥Ø±Ø³Ø§Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯ AI
        ai_analysis = ""
        if openai_client:
            analysis_results = {
                "detection_percentage": detection_percent,
                "status": status,
                "color": color
            }
            try:
                print("ğŸš€ Starting AI assistant analysis...")
                ai_analysis = await analyze_with_ai_assistant(original_path, analysis_results)
                print("ğŸ‰ AI analysis completed successfully!")
            except Exception as ai_error:
                print(f"âŒ AI analysis failed: {ai_error}")
                ai_analysis = f"AI analysis unavailable: {str(ai_error)}"
        else:
            ai_analysis = "ğŸ”¶ AI assistant not available - using neural segmentation only"

         # âœ… Ø­ÙØ¸ Ù…Ø¤Ù‚Øª ÙÙŠ /tmp
        overlay_path = f"/tmp/overlay_{unique_id}.png"
        overlay_image.save(overlay_path)

        # âœ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù†Ø§ØªØ¬Ø© Ø¥Ù„Ù‰ Base64 Ù…Ø¨Ø§Ø´Ø±Ø©
        with open(overlay_path, "rb") as img_file:
            encoded_overlay = base64.b64encode(img_file.read()).decode("utf-8")

        # âœ… Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡ Ù„Ùˆ ØªØ¨ÙŠ Ø§Ù„Ø£ØµÙ„ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        with open(original_path, "rb") as img_file:
            encoded_original = base64.b64encode(img_file.read()).decode("utf-8")

        # âœ… Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙƒÙ€ JSON ÙŠØ­ØªÙˆÙŠ ØµÙˆØ± Base64
        return JSONResponse({
            "success": True,
            "analysis": {
                "detection_percentage": round(detection_percent, 2),
                "status": status,
                "color": color
            },
            "ai_response": ai_analysis,
            "original_image_base64": encoded_original,
            "overlay_image_base64": encoded_overlay,
            "model_used": "Neural Segmentation + AI Assistant" if model else "Demo"
        })

    except Exception as e:
        print(f"âŒ Analysis error: {e}")
        return JSONResponse({"success": False, "error": str(e)})


@app.post("/api/chat")
async def chat_with_ai(request: dict):
    """Chat endpoint for AI conversation with image support"""
    try:
        message = request.get("message", "")
        image_url = request.get("image_url", "")  # ğŸ”¥ Ù‡Ø°Ø§ ÙŠØ£ØªÙŠ ÙƒÙ€ base64 ÙƒØ§Ù…Ù„ Ù…Ù† Frontend

        if not message and not image_url:
            return JSONResponse({"success": False, "error": "Message or image is required"})

        # If OpenAI is not configured, use demo response
        if not openai_client:
            return JSONResponse({
                "success": True,
                "response": "Hello! I'm your medical AI assistant. Currently in demo mode.",
                "demo_mode": True
            })

        # ğŸ”¥ Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT-4 Vision Ø£Ùˆ GPT-4o Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„ØµÙˆØ±
        try:
            messages = [
                {
                    "role": "system",
                    "content": """You are Dr. Copilot, an advanced medical AI imaging assistant. Your role is to provide detailed, educational analysis of medical images.

âœ… WHAT YOU CAN AND SHOULD DO:
- Describe visible anatomical structures in detail
- Explain imaging features, textures, and patterns
- Identify normal vs. abnormal appearances
- Provide educational insights about medical imaging
- Suggest what medical professionals look for in such scans
- Explain the clinical significance of various findings
- Guide users on appropriate next steps

ğŸ“‹ SPECIFIC ANALYSIS FRAMEWORK:
1. **Image Description**: Describe what you see - anatomy, structures, regions
2. **Feature Analysis**: Note densities, intensities, textures, contrasts  
3. **Pattern Recognition**: Identify any notable patterns or symmetries
4. **Educational Context**: Explain what these features mean in medical terms
5. **Professional Guidance**: Emphasize consultation with healthcare providers

ğŸš« REMINDERS:
- You are an educational assistant, not a diagnostic tool
- Always recommend professional medical consultation
- Focus on descriptive analysis rather than definitive conclusions
- Use clear, accessible language for medical education

Be thorough, descriptive, and educational in your analysis."""
                }
            ]

            # ğŸ”¥ ğŸ”¥ ğŸ”¥ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
            if image_url:
                # âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø¥Ø°Ø§ image_url Ù‡Ùˆ base64 ÙƒØ§Ù…Ù„ (Ù…Ø«Ù„: data:image/png;base64,xxxx)
                # Ù„Ø§ ØªØ¶ÙŠÙ header Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ - Ø£Ø±Ø³Ù„Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ
                final_image_url = image_url

                message_content = [
                    {"type": "text",
                     "text": message or "Please provide a detailed educational analysis of this medical image, describing what you observe and explaining the features in medical context."},
                    {"type": "image_url", "image_url": {"url": final_image_url}}
                ]
            else:
                # Ø¥Ø°Ø§ Ù…Ø§ÙÙŠ ØµÙˆØ±Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… prompt Ù…Ø®ØªÙ„Ù
                enhanced_message = message
                if "image" in message.lower() or "scan" in message.lower() or "mri" in message.lower() or "x-ray" in message.lower():
                    enhanced_message = f"{message} - Please describe what medical professionals typically look for in such images and what various findings might indicate."

                message_content = enhanced_message

            messages.append({
                "role": "user",
                "content": message_content
            })

            print(f"ğŸ“¤ Sending to OpenAI - Message: {message}, Has Image: {bool(image_url)}")

            response = openai_client.chat.completions.create(
                model="gpt-4o",  # âœ… ØªØ£ÙƒØ¯ Ø£Ù†Ù‡ gpt-4o
                messages=messages,
                max_tokens=1500,  # Ø²ÙˆØ¯ Ø§Ù„Ù€ tokens Ø¹Ø´Ø§Ù† Ø±Ø¯ÙˆØ¯ Ø£Ø·ÙˆÙ„
                temperature=0.7
            )

            response_text = response.choices[0].message.content
            print("âœ… OpenAI response received successfully")

            return JSONResponse({
                "success": True,
                "response": response_text,
                "demo_mode": False
            })

        except Exception as e:
            print(f"âŒ OpenAI API error: {e}")
            # ğŸ”¥ Fallback Ø¨Ø³ÙŠØ· Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
            try:
                # Ø¥Ø°Ø§ ÙØ´Ù„ Ø¨Ø³Ø¨Ø¨ Ø§Ù„ØµÙˆØ±Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø¨Ø¯ÙˆÙ†Ù‡Ø§
                fallback_message = message or "Please analyze this medical image"

                response = openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": "You are Dr. Copilot, a medical AI assistant. Provide helpful medical information and educational insights."
                        },
                        {
                            "role": "user",
                            "content": f"{fallback_message} (Note: Detailed image analysis is currently unavailable - please describe what you see for general medical guidance)"
                        }
                    ],
                    max_tokens=1000,
                    temperature=0.7
                )
                response_text = response.choices[0].message.content
                return JSONResponse({
                    "success": True,
                    "response": response_text,
                    "demo_mode": False,
                    "fallback": True
                })
            except Exception as fallback_error:
                print(f"âŒ Fallback also failed: {fallback_error}")
                return JSONResponse({
                    "success": True,
                    "response": "I'm here to help with medical questions. Please describe the image or symptoms in detail for educational guidance. Always consult healthcare professionals for medical concerns.",
                    "demo_mode": True
                })

    except Exception as e:
        print(f"âŒ Chat endpoint error: {e}")
        return JSONResponse({"success": False, "error": str(e)})


if __name__ == "__main__":
    import uvicorn

    print("\n" + "=" * 60)
    print("ğŸ©º Dr. Copilot - Ù†Ø¸Ø§Ù… Ù…ØªÙƒØ§Ù…Ù„")
    print("=" * 60)
    print(f"ğŸ“Š Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {'âœ… Ù…Ø­Ù…Ù„' if model else 'âŒ ØºÙŠØ± Ù…ØªØ§Ø­'}")
    print(f"ğŸ¤– OpenAI API: {'âœ… Ù…ÙØ¹Ù„' if openai_client else 'âŒ Ù…Ø¹Ø·Ù„'}")
    print(f"ğŸ”‘ API Key: {'âœ… Ù…Ø¶Ø¨ÙˆØ·' if api_key and api_key.startswith('sk-') else 'âŒ Ù…ÙÙ‚ÙˆØ¯'}")
    print("=" * 60)
    print("ğŸŒ Ø§Ù„Ø®Ø§Ø¯Ù… ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰: http://localhost:8001")
    print("=" * 60)

    uvicorn.run(app, host="0.0.0.0", port=8080, reload=False)
